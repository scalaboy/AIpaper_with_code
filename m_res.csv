,author,title,summary
0,"[('Hanlei Zhang'), ('Hua Xu'), ('Fei Long'), ('Xin Wang'), ('Kai Gao')]",Unsupervised Multimodal Clustering for Semantics Discovery in Multimodal Utterances,"Discovering the semantics of multimodal utterances is essential for
understanding human language and enhancing human-machine interactions. Existing
methods manifest limitations in leveraging nonverbal information for discerning
complex semantics in unsupervised scenarios. This paper introduces a novel
unsupervised multimodal clustering method (UMC), making a pioneering
contribution to this field. UMC introduces a unique approach to constructing
augmentation views for multimodal data, which are then used to perform
pre-training to establish well-initialized representations for subsequent
clustering. An innovative strategy is proposed to dynamically select
high-quality samples as guidance for representation learning, gauged by the
density of each sample's nearest neighbors. Besides, it is equipped to
automatically determine the optimal value for the top-$K$ parameter in each
cluster to refine sample selection. Finally, both high- and low-quality samples
are used to learn representations conducive to effective clustering. We build
baselines on benchmark multimodal intent and dialogue act datasets. UMC shows
remarkable improvements of 2-6\% scores in clustering metrics over
state-of-the-art methods, marking the first successful endeavor in this domain.
The complete code and data are available at https://github.com/thuiar/UMC."
1,"[('Dayou Du'), ('Yijia Zhang'), ('Shijie Cao'), ('Jiaqi Guo'), ('Ting Cao'), ('Xiaowen Chu'), ('Ningyi Xu')]",BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation,"The upscaling of Large Language Models (LLMs) has yielded impressive advances
in natural language processing, yet it also poses significant deployment
challenges. Weight quantization has emerged as a widely embraced solution to
reduce memory and computational demands. This paper introduces BitDistiller, a
framework that synergizes Quantization-Aware Training (QAT) with Knowledge
Distillation (KD) to boost the performance of LLMs at ultra-low precisions
(sub-4-bit). Specifically, BitDistiller first incorporates a tailored
asymmetric quantization and clipping technique to maximally preserve the
fidelity of quantized weights, and then proposes a novel Confidence-Aware
Kullback-Leibler Divergence (CAKLD) objective, which is employed in a
self-distillation manner to enable faster convergence and superior model
performance. Empirical evaluations demonstrate that BitDistiller significantly
surpasses existing methods in both 3-bit and 2-bit configurations on general
language understanding and complex reasoning benchmarks. Notably, BitDistiller
is shown to be more cost-effective, demanding fewer data and training
resources. The code is available at https://github.com/DD-DuDa/BitDistiller."
2,"[('Shicheng Xu'), ('Liang Pang'), ('Mo Yu'), ('Fandong Meng'), ('Huawei Shen'), ('Xueqi Cheng'), ('Jie Zhou')]",Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation,"Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
incorporating additional information from retrieval. However, studies have
shown that LLMs still face challenges in effectively using the retrieved
information, even ignoring it or being misled by it. The key reason is that the
training of LLMs does not clearly make LLMs learn how to utilize input
retrieved texts with varied quality. In this paper, we propose a novel
perspective that considers the role of LLMs in RAG as ``Information Refiner'',
which means that regardless of correctness, completeness, or usefulness of
retrieved texts, LLMs can consistently integrate knowledge within the retrieved
texts and model parameters to generate the texts that are more concise,
accurate, and complete than the retrieved texts. To this end, we propose an
information refinement training method named InFO-RAG that optimizes LLMs for
RAG in an unsupervised manner. InFO-RAG is low-cost and general across various
tasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse
tasks including Question Answering, Slot-Filling, Language Modeling, Dialogue,
and Code Generation show that InFO-RAG improves the performance of LLaMA2 by an
average of 9.39\% relative points. InFO-RAG also shows advantages in in-context
learning and robustness of RAG."
3,"[('Guanting Dong'), ('Hongyi Yuan'), ('Keming Lu'), ('Chengpeng Li'), ('Mingfeng Xue'), ('Dayiheng Liu'), ('Wei Wang'), ('Zheng Yuan'), ('Chang Zhou'), ('Jingren Zhou')]",How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition,"Large language models (LLMs) with enormous pre-training tokens and parameters
emerge diverse abilities, including math reasoning, code generation, and
instruction following. These abilities are further enhanced by supervised
fine-tuning (SFT). While the open-source community has explored ad-hoc SFT for
enhancing individual capabilities, proprietary LLMs exhibit versatility across
various skills. Therefore, understanding the facilitation of multiple abilities
via SFT is paramount. In this study, we specifically focuses on the interplay
of data composition between mathematical reasoning, code generation, and
general human-aligning abilities during SFT. We propose four intriguing
research questions to explore the association between model performance and
various factors including data amount, composition ratio, model size and SFT
strategies. Our experiments reveal that distinct capabilities scale differently
and larger models generally show superior performance with same amount of data.
Mathematical reasoning and code generation consistently improve with increasing
data amount, whereas general abilities plateau after roughly a thousand
samples. Moreover, we observe data composition appears to enhance various
abilities under limited data conditions, yet can lead to performance conflicts
when data is plentiful. Our findings also suggest the amount of composition
data influences performance more than the composition ratio. In analysis of SFT
strategies, we find that sequentially learning multiple skills risks
catastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT)
strategy offers a promising solution to learn multiple abilities with different
scaling patterns."
4,"[('Eduard Poesina'), ('Cornelia Caragea'), ('Radu Tudor Ionescu')]",A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI: The First Romanian Natural Language Inference Corpus,"Natural language inference (NLI), the task of recognizing the entailment
relationship in sentence pairs, is an actively studied topic serving as a proxy
for natural language understanding. Despite the relevance of the task in
building conversational agents and improving text classification, machine
translation and other NLP tasks, to the best of our knowledge, there is no
publicly available NLI corpus for the Romanian language. To this end, we
introduce the first Romanian NLI corpus (RoNLI) comprising 58K training
sentence pairs, which are obtained via distant supervision, and 6K validation
and test sentence pairs, which are manually annotated with the correct labels.
We conduct experiments with multiple machine learning methods based on distant
learning, ranging from shallow models based on word embeddings to
transformer-based neural networks, to establish a set of competitive baselines.
Furthermore, we improve on the best model by employing a new curriculum
learning strategy based on data cartography. Our dataset and code to reproduce
the baselines are available at https://github.com/Eduard6421/RONLI."
5,"[('Yiren Jian'), ('Tingkai Liu'), ('Yunzhe Tao'), ('Chunhui Zhang'), ('Soroush Vosoughi'), ('Hongxia Yang')]",Expedited Training of Visual Conditioned Language Generation via Redundancy Reduction,"In this paper, we introduce $\text{EVL}_{\text{Gen}}$, a streamlined
framework designed for the pre-training of visually conditioned language
generation models with high computational demands, utilizing frozen pre-trained
large language models (LLMs). The conventional approach in vision-language
pre-training (VLP) typically involves a two-stage optimization process: an
initial resource-intensive phase dedicated to general-purpose vision-language
representation learning, focused on extracting and consolidating relevant
visual features. This is followed by a subsequent phase that emphasizes
end-to-end alignment between visual and linguistic modalities. Our novel
one-stage, single-loss framework bypasses the computationally demanding first
training stage by gradually merging similar visual tokens during training,
while avoiding model collapse caused by single-stage training of BLIP-2 type
models. The gradual merging process effectively condenses visual information
while preserving semantic richness, resulting in rapid convergence without
compromising performance. Our experimental findings demonstrate that our
approach accelerates the training of vision-language models by a factor of 5
without a noticeable impact on overall performance. Furthermore, we illustrate
that our models significantly narrow the performance gap to current
vision-language models using only 1/10 of the data. Finally, we showcase how
our image-text models can seamlessly adapt to video-conditioned language
generation tasks through novel soft attentive temporal token contextualizing
modules. Code is available at \url{https://github.com/yiren-jian/EVLGen}."
6,"[('Yueqi Xie'), ('Minghong Fang'), ('Renjie Pi'), ('Neil Gong')]",GradSafe: Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis,"Large Language Models (LLMs) face threats from jailbreak prompts. Existing
methods for detecting jailbreak prompts are primarily online moderation APIs or
finetuned LLMs. These strategies, however, often require extensive and
resource-intensive data collection and training processes. In this study, we
propose GradSafe, which effectively detects jailbreak prompts by scrutinizing
the gradients of safety-critical parameters in LLMs. Our method is grounded in
a pivotal observation: the gradients of an LLM's loss for jailbreak prompts
paired with compliance response exhibit similar patterns on certain
safety-critical parameters. In contrast, safe prompts lead to different
gradient patterns. Building on this observation, GradSafe analyzes the
gradients from prompts (paired with compliance responses) to accurately detect
jailbreak prompts. We show that GradSafe, applied to Llama-2 without further
training, outperforms Llama Guard, despite its extensive finetuning with a
large dataset, in detecting jailbreak prompts. This superior performance is
consistent across both zero-shot and adaptation scenarios, as evidenced by our
evaluations on ToxicChat and XSTest. The source code is available at
https://github.com/xyq7/GradSafe."
7,"[('Runlong Zhou'), ('Simon S. Du'), ('Beibin Li')]",Reflect-RL: Two-Player Online RL Fine-Tuning for LMs,"As language models (LMs) demonstrate their capabilities in various fields,
their application to tasks requiring multi-round interactions has become
increasingly popular. These tasks usually have complex dynamics, so supervised
fine-tuning (SFT) on a limited offline dataset does not yield good performance.
However, only a few works attempted to directly train the LMs within
interactive decision-making environments. We aim to create an effective
approach to fine-tune LMs with online reinforcement learning (RL) in these
environments. We propose Reflect-RL, a two-player system to fine-tune an LM
using SFT and online RL, where a frozen reflection model (player) assists the
policy model (player). To generate data for the warm-up SFT stage, we use
negative example generation to enhance the error-correction ability of the
reflection model. Furthermore, we designed single-prompt action enumeration and
applied curriculum learning to allow the policy model to learn more
efficiently. Empirically, we verify that Reflect-RL outperforms SFT and online
RL without reflection. Testing results indicate GPT-2 XL 1.56B fine-tuned with
Reflect-RL outperforms larger open-source LMs, such as Mistral 7B. The
benchmarks, dataset, and code involved in this work are publicly available:
https://github.com/zhourunlong/Reflect-RL."
8,"[('Sitao Cheng'), ('Ziyuan Zhuang'), ('Yong Xu'), ('Fangkai Yang'), ('Chaoyun Zhang'), ('Xiaoting Qin'), ('Xiang Huang'), ('Ling Chen'), ('Qingwei Lin'), ('Dongmei Zhang'), ('Saravan Rajmohan'), ('Qi Zhang')]",Can ChatGPT’s Performance be Improved on Verb Metaphor Detection Tasks? Bootstrapping and Combining Tacit Knowledge,"Large Language Models (LLMs) have shown potential in reasoning over
structured environments, e.g., knowledge graph and table. Such tasks typically
require multi-hop reasoning, i.e., match natural language utterance with
instances in the environment. Previous methods leverage LLMs to incrementally
build a reasoning path, where the LLMs either invoke tools or pick up schemas
by step-by-step interacting with the environment. We propose
Reasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently
and faithfully reason over structured environments. In Readi, LLMs initially
generate a reasoning path given a query, and edit the path only when necessary.
We instantiate the path on structured environments and provide feedback to edit
the path if anything goes wrong. Experimental results on three KGQA and two
TableQA datasets show the effectiveness of Readi, significantly surpassing
previous LLM-based methods (by 9.1% Hit@1 on WebQSP, 12.4% on MQA-3H and 9.5%
on WTQ), comparable with state-of-the-art fine-tuned methods (67% on CWQ and
74.7% on WebQSP) and substantially boosting the vanilla LLMs (by 14.9% on CWQ).
Our code will be available on https://aka.ms/readi."
9,"[('Shizhe Diao'), ('Pengcheng Wang'), ('Yong Lin'), ('Tong Zhang')]",Active Prompting with Chain-of-Thought for Large Language Models,"The increasing scale of large language models (LLMs) brings emergent
abilities to various complex tasks requiring reasoning, such as arithmetic and
commonsense reasoning. It is known that the effective design of task-specific
prompts is critical for LLMs' ability to produce high-quality answers. In
particular, an effective approach for complex question-and-answer tasks is
example-based prompting with chain-of-thought (CoT) reasoning, which
significantly improves the performance of LLMs. However, current CoT methods
rely on a fixed set of human-annotated exemplars, which are not necessarily the
most effective examples for different tasks. This paper proposes a new method,
Active-Prompt, to adapt LLMs to different tasks with task-specific example
prompts (annotated with human-designed CoT reasoning). For this purpose, we
propose a solution to the key problem of determining which questions are the
most important and helpful ones to annotate from a pool of task-specific
queries. By borrowing ideas from the related problem of uncertainty-based
active learning, we introduce several metrics to characterize the uncertainty
so as to select the most uncertain questions for annotation. Experimental
results demonstrate the superiority of our proposed method, achieving
state-of-the-art on eight complex reasoning tasks. Further analyses of
different uncertainty metrics, pool sizes, zero-shot learning, and
accuracy-uncertainty relationship demonstrate the effectiveness of our method.
Our code will be available at https://github.com/shizhediao/active-prompt."
10,"[('Haochen Li'), ('Xin Zhou'), ('Zhiqi Shen')]",Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search,"In code search, the Generation-Augmented Retrieval (GAR) framework, which
generates exemplar code snippets to augment queries, has emerged as a promising
strategy to address the principal challenge of modality misalignment between
code snippets and natural language queries, particularly with the demonstrated
code generation capabilities of Large Language Models (LLMs). Nevertheless, our
preliminary investigations indicate that the improvements conferred by such an
LLM-augmented framework are somewhat constrained. This limitation could
potentially be ascribed to the fact that the generated codes, albeit
functionally accurate, frequently display a pronounced stylistic deviation from
the ground truth code in the codebase. In this paper, we extend the
foundational GAR framework and propose a simple yet effective method that
additionally Rewrites the Code (ReCo) within the codebase for style
normalization. Experimental results demonstrate that ReCo significantly boosts
retrieval accuracy across sparse (up to 35.7%), zero-shot dense (up to 27.6%),
and fine-tuned dense (up to 23.6%) retrieval settings in diverse search
scenarios. To further elucidate the advantages of ReCo and stimulate research
in code style normalization, we introduce Code Style Similarity, the first
metric tailored to quantify stylistic similarities in code. Notably, our
empirical findings reveal the inadequacy of existing metrics in capturing
stylistic nuances. The source code and data are available at
\url{https://github.com/Alex-HaochenLi/ReCo}."
11,"[('Shangqing Tu'), ('Yuliang Sun'), ('Yushi Bai'), ('Jifan Yu'), ('Lei Hou'), ('Juanzi Li')]",WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models,"To mitigate the potential misuse of large language models (LLMs), recent
research has developed watermarking algorithms, which restrict the generation
process to leave an invisible trace for watermark detection. Due to the
two-stage nature of the task, most studies evaluate the generation and
detection separately, thereby presenting a challenge in unbiased, thorough, and
applicable evaluations. In this paper, we introduce WaterBench, the first
comprehensive benchmark for LLM watermarks, in which we design three crucial
factors: (1) For benchmarking procedure, to ensure an apples-to-apples
comparison, we first adjust each watermarking method's hyper-parameter to reach
the same watermarking strength, then jointly evaluate their generation and
detection performance. (2) For task selection, we diversify the input and
output length to form a five-category taxonomy, covering $9$ tasks. (3) For
evaluation metric, we adopt the GPT4-Judge for automatically evaluating the
decline of instruction-following abilities after watermarking. We evaluate $4$
open-source watermarks on $2$ LLMs under $2$ watermarking strengths and observe
the common struggles for current methods on maintaining the generation quality.
The code and data are available at https://github.com/THU-KEG/WaterBench."
12,"[('Zhenhua Liu'), ('Tong Zhu'), ('Chuanyuan Tan'), ('Haonan Lu'), ('Bing Liu'), ('Wenliang Chen')]",Probing Language Models for Pre-training Data Detection,"Large Language Models (LLMs) have shown their impressive capabilities, while
also raising concerns about the data contamination problems due to privacy
issues and leakage of benchmark datasets in the pre-training phase. Therefore,
it is vital to detect the contamination by checking whether an LLM has been
pre-trained on the target texts. Recent studies focus on the generated texts
and compute perplexities, which are superficial features and not reliable. In
this study, we propose to utilize the probing technique for pre-training data
detection by examining the model's internal activations. Our method is simple
and effective and leads to more trustworthy pre-training data detection.
Additionally, we propose ArxivMIA, a new challenging benchmark comprising arxiv
abstracts from Computer Science and Mathematics categories. Our experiments
demonstrate that our method outperforms all baselines, and achieves
state-of-the-art performance on both WikiMIA and ArxivMIA, with additional
experiments confirming its efficacy (Our code and dataset are available at
https://github.com/zhliu0106/probing-lm-data)."
13,"[('Huiqiang Jiang'), ('Qianhui Wu'), ('Xufang Luo'), ('Dongsheng Li'), ('Chin-Yew Lin'), ('Yuqing Yang'), ('Lili Qiu')]",LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression,"In long context scenarios, large language models (LLMs) face three main
challenges: higher computational/financial cost, longer latency, and inferior
performance. Some studies reveal that the performance of LLMs depends on both
the density and the position of the key information (question relevant) in the
input prompt. Inspired by these findings, we propose LongLLMLingua for prompt
compression towards improving LLMs' perception of the key information to
simultaneously address the three challenges. We conduct evaluation on a wide
range of long context scenarios including single-/multi-document QA, few-shot
learning, summarization, synthetic tasks, and code completion. The experimental
results show that LongLLMLingua compressed prompt can derive higher performance
with much less cost. The latency of the end-to-end system is also reduced. For
example, on NaturalQuestions benchmark, LongLLMLingua gains a performance boost
of up to 17.1% over the original prompt with ~4x fewer tokens as input to
GPT-3.5-Turbo. It can derive cost savings of \$28.5 and \$27.4 per 1,000
samples from the LongBench and ZeroScrolls benchmark, respectively.
Additionally, when compressing prompts of ~10k tokens at a compression rate of
2x-10x, LongLLMLingua can speed up the end-to-end latency by 1.4x-3.8x. Our
code is available at https://aka.ms/LLMLingua."
14,"[('Tao Sun'), ('Linzheng Chai'), ('Jian Yang'), ('Yuwei Yin'), ('Hongcheng Guo'), ('Jiaheng Liu'), ('Bing Wang'), ('Liqun Yang'), ('Zhoujun Li')]",UniCoder: Scaling Code Large Language Model via Universal Code,"Intermediate reasoning or acting steps have successfully improved large
language models (LLMs) for handling various downstream natural language
processing (NLP) tasks. When applying LLMs for code generation, recent works
mainly focus on directing the models to articulate intermediate
natural-language reasoning steps, as in chain-of-thought (CoT) prompting, and
then output code with the natural language or other structured intermediate
steps. However, such output is not suitable for code translation or generation
tasks since the standard CoT has different logical structures and forms of
expression with the code. In this work, we introduce the universal code
(UniCode) as the intermediate representation. It is a description of algorithm
steps using a mix of conventions of programming languages, such as assignment
operator, conditional operator, and loop. Hence, we collect an instruction
dataset UniCoder-Instruct to train our model UniCoder on multi-task learning
objectives. UniCoder-Instruct comprises natural-language questions, code
solutions, and the corresponding universal code. The alignment between the
intermediate universal code representation and the final code solution
significantly improves the quality of the generated code. The experimental
results demonstrate that UniCoder with the universal code significantly
outperforms the previous prompting methods by a large margin, showcasing the
effectiveness of the structural clues in pseudo-code."
15,"[('Yan Ma'), ('Yu Qiao'), ('Pengfei Liu')]",MoPS: Modular Story Premise Synthesis for Open-Ended Automatic Story Generation,"A story premise succinctly defines a story's main idea, foundation, and
trajectory. It serves as the initial trigger in automatic story generation.
Existing sources of story premises are limited by a lack of diversity, uneven
quality, and high costs that make them difficult to scale. In response, we
introduce Modular Story Premise Synthesis (MoPS) which breaks down story
premises into modules like background and persona for automated design and
generation. MoPS consists of three phases: (1) Precollect a consistent set of
candidates for each module to form a nested dictionary. (2) Extract a key path
from the nested dictionary as the premise design. (3) Instruct an LLM to
integrate the design into a coherent premise sentence. Thorough evaluations
demonstrate that our synthesized premises excel in diversity, fascination,
completeness, and originality compared to those induced from large language
models and captured from public story datasets. Similarly, the extended novels
and scripts generated from our premises also exhibit higher quality. In
supplementary materials, we provide the MoPS code suite, along with 7.6k
generated premises and 1k extended stories. Code:
https://github.com/GAIR-NLP/MoPS."
16,"[('Arnav Chavan'), ('Nahush Lele'), ('Deepak Gupta')]","Surgical Feature-Space Decomposition of LLMs: Why, When and How?","Low-rank approximations, of the weight and feature space can enhance the
performance of deep learning models, whether in terms of improving
generalization or reducing the latency of inference. However, there is no clear
consensus yet on \emph{how}, \emph{when} and \emph{why} these approximations
are helpful for large language models (LLMs). In this work, we empirically
study the efficacy of weight and feature space decomposition in
transformer-based LLMs. We demonstrate that surgical decomposition not only
provides critical insights into the trade-off between compression and language
modelling performance, but also sometimes enhances commonsense reasoning
performance of LLMs. Our empirical analysis identifies specific network
segments that intrinsically exhibit a low-rank structure. Furthermore, we
extend our investigation to the implications of low-rank approximations on
model bias. Overall, our findings offer a novel perspective on optimizing LLMs,
presenting the low-rank approximation not only as a tool for performance
enhancements, but also as a means to potentially rectify biases within these
models. Our code is available at
\href{https://github.com/nyunAI/SFSD-LLM}{GitHub}."
17,"[('Yao Yao'), ('Zuchao Li'), ('Hai Zhao')]",SirLLM: Streaming Infinite Retentive LLM,"As Large Language Models (LLMs) become increasingly prevalent in various
domains, their ability to process inputs of any length and maintain a degree of
memory becomes essential. However, the one-off input of overly long texts is
limited, as studies have shown that when input lengths exceed the LLMs'
pre-trained text length, there is a dramatic decline in text generation
capabilities. Moreover, simply extending the length of pre-training texts is
impractical due to the difficulty in obtaining long text data and the
substantial memory consumption costs this would entail for LLMs. Recent efforts
have employed streaming inputs to alleviate the pressure of excessively long
text inputs, but this approach can significantly impair the model's long-term
memory capabilities.
  Motivated by this challenge, we introduce Streaming Infinite Retentive LLM
(SirLLM), which allows LLMs to maintain longer memory during infinite-length
dialogues without the need for fine-tuning. SirLLM utilizes the Token Entropy
metric and a memory decay mechanism to filter key phrases, endowing LLMs with
both long-lasting and flexible memory. We designed three distinct tasks and
constructed three datasets to measure the effectiveness of SirLLM from various
angles: (1) DailyDialog; (2) Grocery Shopping; (3) Rock-Paper-Scissors. Our
experimental results robustly demonstrate that SirLLM can achieve stable and
significant improvements across different LLMs and tasks, compellingly proving
its effectiveness. When having a coversation, ""A sir could forget himself,"" but
SirLLM never does! Our code is publicly available at
https://github.com/Zoeyyao27/SirLLM"
18,"[('Jintian Zhang'), ('Xin Xu'), ('Ningyu Zhang'), ('Ruibo Liu'), ('Bryan Hooi'), ('Shumin Deng')]",Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View,"As Natural Language Processing (NLP) systems are increasingly employed in
intricate social environments, a pressing query emerges: Can these NLP systems
mirror human-esque collaborative intelligence, in a multi-agent society
consisting of multiple large language models (LLMs)? This paper probes the
collaboration mechanisms among contemporary NLP systems by melding practical
experiments with theoretical insights. We fabricate four unique `societies'
comprised of LLM agents, where each agent is characterized by a specific
`trait' (easy-going or overconfident) and engages in collaboration with a
distinct `thinking pattern' (debate or reflection). Through evaluating these
multi-agent societies on three benchmark datasets, we discern that certain
collaborative strategies not only outshine previous top-tier approaches, but
also optimize efficiency (using fewer API tokens). Moreover, our results
further illustrate that LLM agents manifest human-like social behaviors, such
as conformity and consensus reaching, mirroring foundational social psychology
theories. In conclusion, we integrate insights from social psychology to
contextualize the collaboration of LLM agents, inspiring further investigations
into the collaboration mechanism for LLMs. We commit to sharing our code and
datasets\footnote{\url{https://github.com/zjunlp/MachineSoM}.}, hoping to
catalyze further research in this promising avenue."
19,"[('Haonan Chen'), ('Zhicheng Dou'), ('Kelong Mao'), ('Jiongnan Liu'), ('Ziliang Zhao')]",Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation,"Conversational search utilizes muli-turn natural language contexts to
retrieve relevant passages. Existing conversational dense retrieval models
mostly view a conversation as a fixed sequence of questions and responses,
overlooking the severe data sparsity problem -- that is, users can perform a
conversation in various ways, and these alternate conversations are unrecorded.
Consequently, they often struggle to generalize to diverse conversations in
real-world scenarios. In this work, we propose a framework for generalizing
Conversational dense retrieval via LLM-cognition data Augmentation (ConvAug).
ConvAug first generates multi-level augmented conversations to capture the
diverse nature of conversational contexts. Inspired by human cognition, we
devise a cognition-aware process to mitigate the generation of false positives,
false negatives, and hallucinations. Moreover, we develop a difficulty-adaptive
sample filter that selects challenging samples for complex conversations,
thereby giving the model a larger learning space. A contrastive learning
objective is then employed to train a better conversational context encoder.
Extensive experiments conducted on four public datasets, under both normal and
zero-shot settings, demonstrate the effectiveness, generalizability, and
applicability of ConvAug. The code is released at
https://github.com/haon-chen/ConvAug."
20,"[('Wangtao Sun'), ('Haotian Xu'), ('Xuanqing Yu'), ('Pei Chen'), ('Shizhu He'), ('Jun Zhao'), ('Kang Liu')]",ItD: Large Language Models Can Teach Themselves Induction through Deduction,"Although Large Language Models (LLMs) are showing impressive performance on a
wide range of Natural Language Processing tasks, researchers have found that
they still have limited ability to conduct induction. Recent works mainly adopt
``post processes'' paradigms to improve the performance of LLMs on induction
(e.g., the hypothesis search & refinement methods), but their performance is
still constrained by the inherent inductive capability of the LLMs. In this
paper, we propose a novel framework, Induction through Deduction (ItD), to
enable the LLMs to teach themselves induction through deduction. The ItD
framework is composed of two main components: a Deductive Data Generation
module to generate induction data and a Naive Bayesian Induction module to
optimize the fine-tuning and decoding of LLMs. Our empirical results showcase
the effectiveness of ItD on two induction benchmarks, achieving relative
performance improvement of 36% and 10% compared with previous state-of-the-art,
respectively. Our ablation study verifies the effectiveness of two key modules
of ItD. We also verify the effectiveness of ItD across different LLMs and
deductors. The data and code of this paper can be found at
https://anonymous.4open.science/r/ItD-E844."
21,"[('Hao Zhou'), ('Tiancheng Shen'), ('Xu Yang'), ('Hai Huang'), ('Xiangtai Li'), ('Lu Qi'), ('Ming-Hsuan Yang')]",Rethinking Task-Oriented Dialogue Systems: From Complex Modularity to Zero-Shot Autonomous Agent,"In this paper, we highlight a problem of evaluation metrics adopted in the
open-vocabulary segmentation. That is, the evaluation process still heavily
relies on closed-set metrics on zero-shot or cross-dataset pipelines without
considering the similarity between predicted and ground truth categories. To
tackle this issue, we first survey eleven similarity measurements between two
categorical words using WordNet linguistics statistics, text embedding, and
language models by comprehensive quantitative analysis and user study. Built
upon those explored measurements, we designed novel evaluation metrics, namely
Open mIoU, Open AP, and Open PQ, tailored for three open-vocabulary
segmentation tasks. We benchmarked the proposed evaluation metrics on 12
open-vocabulary methods of three segmentation tasks. Even though the relative
subjectivity of similarity distance, we demonstrate that our metrics can still
well evaluate the open ability of the existing open-vocabulary segmentation
methods. We hope that our work can bring with the community new thinking about
how to evaluate the open ability of models. The evaluation code is released in
github."
22,"[('Mathieu Ravaut'), ('Aixin Sun'), ('Nancy F. Chen'), ('Shafiq Joty')]",On Context Utilization in Summarization with Large Language Models,"Large language models (LLMs) excel in abstractive summarization tasks,
delivering fluent and pertinent summaries. Recent advancements have extended
their capabilities to handle long-input contexts, exceeding 100k tokens.
However, in question answering, language models exhibit uneven utilization of
their input context. They tend to favor the initial and final segments,
resulting in a U-shaped performance pattern concerning where the answer is
located within the input. This bias raises concerns, particularly in
summarization where crucial content may be dispersed throughout the source
document(s). Besides, in summarization, mapping facts from the source to the
summary is not trivial as salient content is usually re-phrased. In this paper,
we conduct the first comprehensive study on context utilization and position
bias in summarization. Our analysis encompasses 6 LLMs, 10 datasets, and 5
evaluation metrics. We introduce a new evaluation benchmark called MiddleSum on
the which we benchmark two alternative inference methods to alleviate position
bias: hierarchical summarization and incremental summarization. Our code and
data can be found here: https://github.com/ntunlp/MiddleSum."
23,"[('Shuofei Qiao'), ('Ningyu Zhang'), ('Runnan Fang'), ('Yujie Luo'), ('Wangchunshu Zhou'), ('Yuchen Eleanor Jiang'), ('Chengfei Lv'), ('Huajun Chen')]",AutoAct: Automatic Agent Learning from Scratch for QA via Self-Planning,"Language agents have achieved considerable performance on various complex
question-answering tasks by planning with external tools. Despite the incessant
exploration in this field, existing language agent systems still struggle with
costly, non-reproducible data reliance and face the challenge of compelling a
single model for multiple functions. To this end, we introduce AutoAct, an
automatic agent learning framework for QA that does not rely on large-scale
annotated data and synthetic planning trajectories from closed-source models
(e.g., GPT-4). Given limited data with a tool library, AutoAct first
automatically synthesizes planning trajectories without any assistance from
humans or strong closed-source models. Then, AutoAct leverages a
division-of-labor strategy to automatically differentiate based on the target
task information and synthesized trajectories, producing a sub-agent group to
complete the task. We conduct comprehensive experiments with different LLMs,
which demonstrates that AutoAct yields better or parallel performance compared
to various strong baselines. Further analysis demonstrates the effectiveness of
the division-of-labor strategy, with the trajectory quality generated by
AutoAct generally outperforming that of others. Code will be available at
https://github.com/zjunlp/AutoAct."
24,"[('Yushi Bai'), ('Xin Lv'), ('Jiajie Zhang'), ('Hongchang Lyu'), ('Jiankai Tang'), ('Zhidian Huang'), ('Zhengxiao Du'), ('Xiao Liu'), ('Aohan Zeng'), ('Lei Hou'), ('Yuxiao Dong'), ('Jie Tang'), ('Juanzi Li')]","LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding","Although large language models (LLMs) demonstrate impressive performance for
many language tasks, most of them can only handle texts a few thousand tokens
long, limiting their applications on longer sequence inputs, such as books,
reports, and codebases. Recent works have proposed methods to improve LLMs'
long context capabilities by extending context windows and more sophisticated
memory mechanisms. However, comprehensive benchmarks tailored for evaluating
long context understanding are lacking. In this paper, we introduce LongBench,
the first bilingual, multi-task benchmark for long context understanding,
enabling a more rigorous evaluation of long context understanding. LongBench
comprises 21 datasets across 6 task categories in both English and Chinese,
with an average length of 6,711 words (English) and 13,386 characters
(Chinese). These tasks cover key long-text application areas including
single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks,
and code completion. All datasets in LongBench are standardized into a unified
format, allowing for effortless automatic evaluation of LLMs. Upon
comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial
model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still
struggles on longer contexts. (2) Scaled position embedding and fine-tuning on
longer sequences lead to substantial improvement on long context understanding.
(3) Context compression technique such as retrieval brings improvement for
model with weak ability on long contexts, but the performance still lags behind
models that have strong long context understanding capability. The code and
datasets are available at https://github.com/THUDM/LongBench."
25,"[('Junjie Zhou'), ('Zheng Liu'), ('Shitao Xiao'), ('Bo Zhao'), ('Yongping Xiong')]",VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval,"Multi-modal retrieval becomes increasingly popular in practice. However, the
existing retrievers are mostly text-oriented, which lack the capability to
process visual information. Despite the presence of vision-language models like
CLIP, the current methods are severely limited in representing the text-only
and image-only data. In this work, we present a new embedding model VISTA for
universal multi-modal retrieval. Our work brings forth threefold technical
contributions. Firstly, we introduce a flexible architecture which extends a
powerful text encoder with the image understanding capability by introducing
visual token embeddings. Secondly, we develop two data generation strategies,
which bring high-quality composed image-text to facilitate the training of the
embedding model. Thirdly, we introduce a multi-stage training algorithm, which
first aligns the visual token embedding with the text encoder using massive
weakly labeled data, and then develops multi-modal representation capability
using the generated composed image-text data. In our experiments, VISTA
achieves superior performances across a variety of multi-modal retrieval tasks
in both zero-shot and supervised settings. Our model, data, and source code are
available at https://github.com/FlagOpen/FlagEmbedding."
26,"[('Houxing Ren'), ('Mingjie Zhan'), ('Zhongyuan Wu'), ('Hongsheng Li')]",Empowering Character-level Text Infilling by Eliminating Sub-Tokens,"In infilling tasks, sub-tokens, representing instances where a complete token
is segmented into two parts, often emerge at the boundaries of prefixes,
middles, and suffixes. Traditional methods focused on training models at the
token level, leading to sub-optimal performance in character-level infilling
tasks during the inference stage. Alternately, some approaches considered
character-level infilling, but they relied on predicting sub-tokens in
inference, yet this strategy diminished ability in character-level infilling
tasks due to the large perplexity of the model on sub-tokens. In this paper, we
introduce FIM-SE, which stands for Fill-In-the-Middle with both Starting and
Ending character constraints. The proposed method addresses character-level
infilling tasks by utilizing a line-level format to avoid predicting any
sub-token in inference. In addition, we incorporate two special tokens to
signify the rest of the incomplete lines, thereby enhancing generation
guidance. Extensive experiments demonstrate that our proposed approach
surpasses previous methods, offering a significant advantage. Code is available
at https://github.com/SenseLLM/FIM-SE."
27,"[('Aoxiong Yin'), ('Haoyuan Li'), ('Kai Shen'), ('Siliang Tang'), ('Yueting Zhuang')]",T2S-GPT: Dynamic Vector Quantization for Autoregressive Sign Language Production from Text,"In this work, we propose a two-stage sign language production (SLP) paradigm
that first encodes sign language sequences into discrete codes and then
autoregressively generates sign language from text based on the learned
codebook. However, existing vector quantization (VQ) methods are fixed-length
encodings, overlooking the uneven information density in sign language, which
leads to under-encoding of important regions and over-encoding of unimportant
regions. To address this issue, we propose a novel dynamic vector quantization
(DVA-VAE) model that can dynamically adjust the encoding length based on the
information density in sign language to achieve accurate and compact encoding.
Then, a GPT-like model learns to generate code sequences and their
corresponding durations from spoken language text. Extensive experiments
conducted on the PHOENIX14T dataset demonstrate the effectiveness of our
proposed method. To promote sign language research, we propose a new large
German sign language dataset, PHOENIX-News, which contains 486 hours of sign
language videos, audio, and transcription texts.Experimental analysis on
PHOENIX-News shows that the performance of our model can be further improved by
increasing the size of the training data. Our project homepage is
https://t2sgpt-demo.yinaoxiong.cn."
28,"[('Tongyao Zhu'), ('Qian Liu'), ('Liang Pang'), ('Zhengbao Jiang'), ('Min-Yen Kan'), ('Min Lin')]",Beyond Memorization: The Challenge of Random Memory Access in Language Models,"Recent developments in Language Models (LMs) have shown their effectiveness
in NLP tasks, particularly in knowledge-intensive tasks. However, the
mechanisms underlying knowledge storage and memory access within their
parameters remain elusive. In this paper, we investigate whether a generative
LM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through
carefully-designed synthetic tasks, covering the scenarios of full recitation,
selective recitation and grounded question answering, we reveal that LMs manage
to sequentially access their memory while encountering challenges in randomly
accessing memorized content. We find that techniques including recitation and
permutation improve the random memory access capability of LMs. Furthermore, by
applying this intervention to realistic scenarios of open-domain question
answering, we validate that enhancing random access by recitation leads to
notable improvements in question answering. The code to reproduce our
experiments can be found at https://github.com/sail-sg/lm-random-memory-access."
29,"[('Bruce W. Lee'), ('JaeHyuk Lim')]",Language Models Don’t Learn the Physical Manifestation of Language,"We argue that language-only models don't learn the physical manifestation of
language. We present an empirical investigation of visual-auditory properties
of language through a series of tasks, termed H-Test. These tasks highlight a
fundamental gap between human linguistic understanding and the sensory-deprived
linguistic understanding of LLMs. In support of our hypothesis, 1. deliberate
reasoning (Chain-of-Thought), 2. few-shot examples, or 3. stronger LLM from the
same model family (LLaMA 2 13B -> LLaMA 2 70B) has no significant effect on
H-Test performance.
  We bring in the philosophical case of Mary, who learns about the world in a
sensory-deprived environment as a useful conceptual framework to understand how
language-only models learn about the world (Jackson, 1986). Our experiments
show that some of the strongest proprietary LLMs stay near random chance
baseline accuracy of 50%, highlighting the limitations of linguistic knowledge
acquired in the absence of sensory experience. Our code and data are available
at <github.com/brucewlee/h-test>."
30,"[('Sara Papi'), ('Marco Gaido'), ('Andrea Pilzer'), ('Matteo Negri')]",When Good and Reproducible Results are a Giant with Feet of Clay: The Importance of Software Quality in NLP,"Despite its crucial role in research experiments, code correctness is often
presumed only on the basis of the perceived quality of results. This assumption
comes with the risk of erroneous outcomes and potentially misleading findings.
To address this issue, we posit that the current focus on reproducibility
should go hand in hand with the emphasis on software quality. We present a case
study in which we identify and fix three bugs in widely used implementations of
the state-of-the-art Conformer architecture. Through experiments on speech
recognition and translation in various languages, we demonstrate that the
presence of bugs does not prevent the achievement of good and reproducible
results, which however can lead to incorrect conclusions that potentially
misguide future research. As a countermeasure, we propose a Code-quality
Checklist and release pangoliNN, a library dedicated to testing neural models,
with the goal of promoting coding best practices and improving research
software quality within the NLP community."
31,"[('Yebin Lee'), ('Imseong Park'), ('Myungjoo Kang')]",FLEUR: An Explainable Reference-Free Evaluation Metric for Image Captioning Using a Large Multimodal Model,"Most existing image captioning evaluation metrics focus on assigning a single
numerical score to a caption by comparing it with reference captions. However,
these methods do not provide an explanation for the assigned score. Moreover,
reference captions are expensive to acquire. In this paper, we propose FLEUR,
an explainable reference-free metric to introduce explainability into image
captioning evaluation metrics. By leveraging a large multimodal model, FLEUR
can evaluate the caption against the image without the need for reference
captions, and provide the explanation for the assigned score. We introduce
score smoothing to align as closely as possible with human judgment and to be
robust to user-defined grading criteria. FLEUR achieves high correlations with
human judgment across various image captioning evaluation benchmarks and
reaches state-of-the-art results on Flickr8k-CF, COMPOSITE, and Pascal-50S
within the domain of reference-free evaluation metrics. Our source code and
results are publicly available at: https://github.com/Yebin46/FLEUR."
32,"[('Zhenlong Dai'), ('Chang Yao'), ('WenKang Han'), ('Ying Yuan'), ('Zhipeng Gao'), ('Jingyuan Chen')]",MPCoder: Multi-user Personalized Code Generator with Explicit and Implicit Style Representation Learning,"Large Language Models (LLMs) have demonstrated great potential for assisting
developers in their daily development. However, most research focuses on
generating correct code, how to use LLMs to generate personalized code has
seldom been investigated. To bridge this gap, we proposed MPCoder (Multi-user
Personalized Code Generator) to generate personalized code for multiple users.
To better learn coding style features, we utilize explicit coding style
residual learning to capture the syntax code style standards and implicit style
learning to capture the semantic code style conventions. We train a multi-user
style adapter to better differentiate the implicit feature representations of
different users through contrastive learning, ultimately enabling personalized
code generation for multiple users. We further propose a novel evaluation
metric for estimating similarities between codes of different coding styles.
The experimental results show the effectiveness of our approach for this novel
task."
33,"[('Chaoqun He'), ('Renjie Luo'), ('Yuzhuo Bai'), ('Shengding Hu'), ('Zhen Leng Thai'), ('Junhao Shen'), ('Jinyi Hu'), ('Xu Han'), ('Yujie Huang'), ('Yuxiang Zhang'), ('Jie Liu'), ('Lei Qi'), ('Zhiyuan Liu'), ('Maosong Sun')]",OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems,"Recent advancements have seen Large Language Models (LLMs) and Large
Multimodal Models (LMMs) surpassing general human capabilities in various
tasks, approaching the proficiency level of human experts across multiple
domains. With traditional benchmarks becoming less challenging for these
models, new rigorous challenges are essential to gauge their advanced
abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual
multimodal scientific benchmark, featuring 8,476 problems from Olympiad-level
mathematics and physics competitions, including the Chinese college entrance
exam. Each problem is detailed with expert-level annotations for step-by-step
reasoning. Evaluating top-tier models on OlympiadBench, we implement a
comprehensive assessment methodology to accurately evaluate model responses.
Notably, the best-performing model, GPT-4V, attains an average score of 17.97%
on OlympiadBench, with a mere 10.74% in physics, highlighting the benchmark
rigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V
points out prevalent issues with hallucinations, knowledge omissions, and
logical fallacies. We hope that our challenging benchmark can serve as a
valuable resource for helping future AGI research endeavors. The data and
evaluation code are available at \url{https://github.com/OpenBMB/OlympiadBench}"
34,"[('Weidong Guo'), ('Jiuding Yang'), ('Kaitong Yang'), ('Xiangyang Li'), ('Zhuwei Rao'), ('Yu Xu'), ('Di Niu')]",Instruction Fusion: Advancing Prompt Evolution through Hybridization,"The fine-tuning of Large Language Models (LLMs) specialized in code
generation has seen notable advancements through the use of open-domain coding
queries. Despite the successes, existing methodologies like Evol-Instruct
encounter performance limitations, impeding further enhancements in code
generation tasks. This paper examines the constraints of existing prompt
evolution techniques and introduces a novel approach, Instruction Fusion (IF).
IF innovatively combines two distinct prompts through a hybridization process,
thereby enhancing the evolution of training prompts for code LLMs. Our
experimental results reveal that the proposed novel method effectively
addresses the shortcomings of prior methods, significantly improving the
performance of Code LLMs across five code generation benchmarks, namely
HumanEval, HumanEval+, MBPP, MBPP+ and MultiPL-E, which underscore the
effectiveness of Instruction Fusion in advancing the capabilities of LLMs in
code generation."
35,"[('Yixin Chen'), ('Shuai Zhang'), ('Boran Han'), ('Tong He'), ('Bo Li')]",CaMML: Context-Aware Multimodal Learner for Large Models,"In this work, we introduce Context-Aware MultiModal Learner (CaMML), for
tuning large multimodal models (LMMs). CaMML, a lightweight module, is crafted
to seamlessly integrate multimodal contextual samples into large models,
thereby empowering the model to derive knowledge from analogous,
domain-specific, up-to-date information and make grounded inferences.
Importantly, CaMML is highly scalable and can efficiently handle lengthy
multimodal context examples owing to its hierarchical design. Based on CaMML,
we have developed two multimodal models, CaMML-7B and CaMML-13B, that have
shown exceptional performance across an array of benchmark datasets for
multimodal tasks. Remarkably, CaMML-13B achieves the state-of-the-art
performance on over ten widely recognized multimodal benchmark datasets,
surpassing LLaVA-1.5 (13B) with a noticeable margin, without integration of any
external resources. Moreover, we have conducted extensive ablative studies to
inspect the inner workings of CaMML and performed qualitative analyses to
showcase its effectiveness in handling real-world challenging cases. Code and
models are available at: https://github.com/amazon-science/camml."
36,"[('Lizhou Fan'), ('Wenyue Hua'), ('Lingyao Li'), ('Haoyang Ling'), ('Yongfeng Zhang')]",NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes,"Complex reasoning ability is one of the most important features of current
LLMs, which has also been leveraged to play an integral role in complex
decision-making tasks. Therefore, the investigation into the reasoning
capabilities of Large Language Models (LLMs) is critical: numerous benchmarks
have been established to assess the reasoning abilities of LLMs. However,
current benchmarks are inadequate in offering a rigorous evaluation of the full
extent of reasoning abilities that LLMs are capable of achieving. They are also
prone to the risk of overfitting, as these benchmarks, being publicly
accessible and static, allow models to potentially tailor their responses to
specific benchmark metrics, thereby inflating their performance. Addressing
these limitations, our research introduces a new benchmark, named NPHardEval.
This benchmark is designed to evaluate the reasoning abilities of LLMs across a
broad spectrum of 900 algorithmic questions, extending up to the NP-Hard
complexity class. These questions are meticulously chosen to represent a wide
range of complexity class below the NP-hard complexity class, offering a
rigorous measure of the reasoning ability of LLMs. Through this study, we shed
light on the current state of reasoning in LLMs, providing an objective and
rigorous perspective through the comparison of LLMs' performance across complex
classes. Moreover, this benchmark is designed with a dynamic update mechanism,
where the datapoints are refreshed on a monthly basis. Such regular updates
play a crucial role in mitigating the risk of LLMs overfitting to the
benchmark, promoting a more accurate and reliable assessment of their reasoning
capabilities. The benchmark dataset and code of NPHardEval are available at
https://github.com/casmlab/NPHardEval."
37,"[('Zihan Liao'), ('Hang Yu'), ('Jianguo Li'), ('Jun Wang'), ('Wei Zhang')]",D2LLM: Decomposed and Distilled Large Language Models for Semantic Search,"The key challenge in semantic search is to create models that are both
accurate and efficient in pinpointing relevant sentences for queries. While
BERT-style bi-encoders excel in efficiency with pre-computed embeddings, they
often miss subtle nuances in search tasks. Conversely, GPT-style LLMs with
cross-encoder designs capture these nuances but are computationally intensive,
hindering real-time applications. In this paper, we present D2LLMs-Decomposed
and Distilled LLMs for semantic search-that combines the best of both worlds.
We decompose a cross-encoder into an efficient bi-encoder integrated with
Pooling by Multihead Attention and an Interaction Emulation Module, achieving
nuanced understanding and pre-computability. Knowledge from the LLM is
distilled into this model using contrastive, rank, and feature imitation
techniques. Our experiments show that D2LLM surpasses five leading baselines in
terms of all metrics across three tasks, particularly improving NLI task
performance by at least 6.45%. The source code is available at
https://github.com/codefuse-ai/D2LLM."
38,"[('Kuo Liao'), ('Shuang Li'), ('Meng Zhao'), ('Liqun Liu'), ('Mengge Xue'), ('Zhenyu Hu'), ('Honglin Han'), ('Chengguo Yin')]",Enhancing Reinforcement Learning with Label-Sensitive Reward for Natural Language Understanding,"Recent strides in large language models (LLMs) have yielded remarkable
performance, leveraging reinforcement learning from human feedback (RLHF) to
significantly enhance generation and alignment capabilities. However, RLHF
encounters numerous challenges, including the objective mismatch issue, leading
to suboptimal performance in Natural Language Understanding (NLU) tasks. To
address this limitation, we propose a novel Reinforcement Learning framework
enhanced with Label-sensitive Reward (RLLR) to amplify the performance of LLMs
in NLU tasks. By incorporating label-sensitive pairs into reinforcement
learning, our method aims to adeptly capture nuanced label-sensitive semantic
features during RL, thereby enhancing natural language understanding.
Experiments conducted on five diverse foundation models across eight tasks
showcase promising results. In comparison to Supervised Fine-tuning models
(SFT), RLLR demonstrates an average performance improvement of 1.54%. Compared
with RLHF models, the improvement averages at 0.69%. These results reveal the
effectiveness of our method for LLMs in NLU tasks. Code and data available at:
https://github.com/MagiaSN/ACL2024_RLLR."
39,"[('Shiyi Zhu'), ('Jing Ye'), ('Wei Jiang'), ('Siqiao Xue'), ('Qi Zhang'), ('Yifan Wu'), ('Jianguo Li')]",CoCA: Fusing Position Embedding with Collinear Constrained Attention in Transformers for Long Context Window Extending,"Self-attention and position embedding are two key modules in
transformer-based Large Language Models (LLMs). However, the potential
relationship between them is far from well studied, especially for long context
window extending. In fact, anomalous behaviors harming long context
extrapolation exist between Rotary Position Embedding (RoPE) and vanilla
self-attention unveiled by our work. To address this issue, we propose a novel
attention mechanism, CoCA (Collinear Constrained Attention). Specifically, we
enforce a collinear constraint between $Q$ and $K$ to seamlessly integrate RoPE
and self-attention. While only adding minimal computational and spatial
complexity, this integration significantly enhances long context window
extrapolation ability. We provide an optimized implementation, making it a
drop-in replacement for any existing transformer-based models. Extensive
experiments show that CoCA performs extraordinarily well in extending context
windows. A CoCA-based GPT model, trained with a context length of 512, can
seamlessly extend the context window up to 32K (60$\times$), without any
fine-tuning. Additionally, by dropping CoCA in LLaMA-7B, we achieve
extrapolation up to 32K within only 2K training length. Our code is publicly
available at: https://github.com/codefuse-ai/Collinear-Constrained-Attention"
40,"[('Salman Elgamal'), ('Ossama Obeid'), ('Tameem Kabbani'), ('Go Inoue'), ('Nizar Habash')]",Arabic Diacritics in the Wild: Exploiting Opportunities for Improved Diacritization,"The widespread absence of diacritical marks in Arabic text poses a
significant challenge for Arabic natural language processing (NLP). This paper
explores instances of naturally occurring diacritics, referred to as
""diacritics in the wild,"" to unveil patterns and latent information across six
diverse genres: news articles, novels, children's books, poetry, political
documents, and ChatGPT outputs. We present a new annotated dataset that maps
real-world partially diacritized words to their maximal full diacritization in
context. Additionally, we propose extensions to the analyze-and-disambiguate
approach in Arabic NLP to leverage these diacritics, resulting in notable
improvements. Our contributions encompass a thorough analysis, valuable
datasets, and an extended diacritization algorithm. We release our code and
datasets as open source."
41,"[('Junhao Zheng'), ('Shengjie Qiu'), ('Qianli Ma')]",Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models,"Incremental Learning (IL) has been a long-standing problem in both vision and
Natural Language Processing (NLP) communities. In recent years, as Pre-trained
Language Models (PLMs) have achieved remarkable progress in various NLP
downstream tasks, utilizing PLMs as backbones has become a common practice in
recent research of IL in NLP. Most assume that catastrophic forgetting is the
biggest obstacle to achieving superior IL performance and propose various
techniques to overcome this issue. However, we find that this assumption is
problematic. Specifically, we revisit more than 20 methods on four
classification tasks (Text Classification, Intent Classification, Relation
Extraction, and Named Entity Recognition) under the two most popular IL
settings (Class-Incremental and Task-Incremental) and reveal that most of them
severely underestimate the inherent anti-forgetting ability of PLMs. Based on
the observation, we propose a frustratingly easy method called SEQ* for IL with
PLMs. The results show that SEQ* has competitive or superior performance
compared to state-of-the-art (SOTA) IL methods and requires considerably less
trainable parameters and training time. These findings urge us to revisit the
IL with PLMs and encourage future studies to have a fundamental understanding
of the catastrophic forgetting in PLMs. The data, code and scripts are publicly
available at
https://github.com/zzz47zzz/codebase-for-incremental-learning-with-llm."
42,"[('Kexin Wang'), ('Nils Reimers'), ('Iryna Gurevych')]",DAPR: A Benchmark on Document-Aware Passage Retrieval,"The work of neural retrieval so far focuses on ranking short texts and is
challenged with long documents. There are many cases where the users want to
find a relevant passage within a long document from a huge corpus, e.g.
Wikipedia articles, research papers, etc. We propose and name this task
\emph{Document-Aware Passage Retrieval} (DAPR). While analyzing the errors of
the State-of-The-Art (SoTA) passage retrievers, we find the major errors
(53.5\%) are due to missing document context. This drives us to build a
benchmark for this task including multiple datasets from heterogeneous domains.
In the experiments, we extend the SoTA passage retrievers with document context
via (1) hybrid retrieval with BM25 and (2) contextualized passage
representations, which inform the passage representation with document context.
We find despite that hybrid retrieval performs the strongest on the mixture of
the easy and the hard queries, it completely fails on the hard queries that
require document-context understanding. On the other hand, contextualized
passage representations (e.g. prepending document titles) achieve good
improvement on these hard queries, but overall they also perform rather poorly.
Our created benchmark enables future research on developing and comparing
retrieval systems for the new task. The code and the data are available at
https://github.com/UKPLab/arxiv2023-dapr."
43,"[('Daniel Reich'), ('Tanja Schultz')]",Uncovering the Full Potential of Visual Grounding Methods in VQA,"Visual Grounding (VG) methods in Visual Question Answering (VQA) attempt to
improve VQA performance by strengthening a model's reliance on
question-relevant visual information. The presence of such relevant information
in the visual input is typically assumed in training and testing. This
assumption, however, is inherently flawed when dealing with imperfect image
representations common in large-scale VQA, where the information carried by
visual features frequently deviates from expected ground-truth contents. As a
result, training and testing of VG-methods is performed with largely inaccurate
data, which obstructs proper assessment of their potential benefits. In this
study, we demonstrate that current evaluation schemes for VG-methods are
problematic due to the flawed assumption of availability of relevant visual
information. Our experiments show that these methods can be much more effective
when evaluation conditions are corrected. Code is provided on GitHub."
44,"[('Chun Liu'), ('Hongguang Zhang'), ('Kainan Zhao'), ('Xinghai Ju'), ('Lin Yang')]",LLMEmbed: Rethinking Lightweight LLM’s Genuine Function in Text Classification,"With the booming of Large Language Models (LLMs), prompt-learning has become
a promising method mainly researched in various research areas. Recently, many
attempts based on prompt-learning have been made to improve the performance of
text classification. However, most of these methods are based on heuristic
Chain-of-Thought (CoT), and tend to be more complex but less efficient. In this
paper, we rethink the LLM-based text classification methodology, propose a
simple and effective transfer learning strategy, namely LLMEmbed, to address
this classical but challenging task. To illustrate, we first study how to
properly extract and fuse the text embeddings via various lightweight LLMs at
different network depths to improve their robustness and discrimination, then
adapt such embeddings to train the classifier. We perform extensive experiments
on publicly available datasets, and the results show that LLMEmbed achieves
strong performance while enjoys low training overhead using lightweight LLM
backbones compared to recent methods based on larger LLMs, i.e. GPT-3, and
sophisticated prompt-based strategies. Our LLMEmbed achieves adequate accuracy
on publicly available benchmarks without any fine-tuning while merely use 4%
model parameters, 1.8% electricity consumption and 1.5% runtime compared to its
counterparts. Code is available at:
https://github.com/ChunLiu-cs/LLMEmbed-ACL2024."
45,"[('Wenda Xu'), ('Guanglei Zhu'), ('Xuandong Zhao'), ('Liangming Pan'), ('Lei Li'), ('William Yang Wang')]",Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement,"Recent studies show that large language models (LLMs) improve their
performance through self-feedback on certain tasks while degrade on others. We
discovered that such a contrary is due to LLM's bias in evaluating their own
output. In this paper, we formally define LLM's self-bias - the tendency to
favor its own generation - using two statistics. We analyze six LLMs (GPT-4,
GPT-3.5, Gemini, LLaMA2, Mixtral and DeepSeek) on translation, constrained text
generation, and mathematical reasoning tasks. We find that self-bias is
prevalent in all examined LLMs across multiple languages and tasks. Our
analysis reveals that while the self-refine pipeline improves the fluency and
understandability of model outputs, it further amplifies self-bias. To mitigate
such biases, we discover that larger model size and external feedback with
accurate assessment can significantly reduce bias in the self-refine pipeline,
leading to actual performance improvement in downstream tasks. The code and
data are released at https://github.com/xu1998hz/llm_self_bias."
46,"[('Yinhao Bai'), ('Yalan Xie'), ('Xiaoyi Liu'), ('Yuhua Zhao'), ('Zhixin Han'), ('Mengting Hu'), ('Hang Gao'), ('Renhong Cheng')]",BvSP: Broad-view Soft Prompting for Few-Shot Aspect Sentiment Quad Prediction,"Aspect sentiment quad prediction (ASQP) aims to predict four aspect-based
elements, including aspect term, opinion term, aspect category, and sentiment
polarity. In practice, unseen aspects, due to distinct data distribution,
impose many challenges for a trained neural model. Motivated by this, this work
formulates ASQP into the few-shot scenario, which aims for fast adaptation in
real applications. Therefore, we first construct a few-shot ASQP dataset (FSQP)
that contains richer categories and is more balanced for the few-shot study.
Moreover, recent methods extract quads through a generation paradigm, which
involves converting the input sentence into a templated target sequence.
However, they primarily focus on the utilization of a single template or the
consideration of different template orders, thereby overlooking the
correlations among various templates. To tackle this issue, we further propose
a Broadview Soft Prompting (BvSP) method that aggregates multiple templates
with a broader view by taking into account the correlation between the
different templates. Specifically, BvSP uses the pre-trained language model to
select the most relevant k templates with Jensen-Shannon divergence. BvSP
further introduces soft prompts to guide the pre-trained language model using
the selected templates. Then, we aggregate the results of multi-templates by
voting mechanism. Empirical results demonstrate that BvSP significantly
outperforms the stateof-the-art methods under four few-shot settings and other
public datasets. Our code and dataset are available at
https://github.com/byinhao/BvSP."
47,"[('Subba Reddy Oota'), ('Emin Çelik'), ('Fatma Deniz'), ('Mariya Toneva')]",Speech language models lack important brain-relevant semantics,"Despite known differences between reading and listening in the brain, recent
work has shown that text-based language models predict both text-evoked and
speech-evoked brain activity to an impressive degree. This poses the question
of what types of information language models truly predict in the brain. We
investigate this question via a direct approach, in which we systematically
remove specific low-level stimulus features (textual, speech, and visual) from
language model representations to assess their impact on alignment with fMRI
brain recordings during reading and listening. Comparing these findings with
speech-based language models reveals starkly different effects of low-level
features on brain alignment. While text-based models show reduced alignment in
early sensory regions post-removal, they retain significant predictive power in
late language regions. In contrast, speech-based models maintain strong
alignment in early auditory regions even after feature removal but lose all
predictive power in late language regions. These results suggest that
speech-based models provide insights into additional information processed by
early auditory regions, but caution is needed when using them to model
processing in late language regions. We make our code publicly available.
[https://github.com/subbareddy248/speech-llm-brain]"
48,"[('Zekun Li'), ('Zhiyu Zoey Chen'), ('Mike Ross'), ('Patrick Huber'), ('Seungwhan Moon'), ('Zhaojiang Lin'), ('Xin Luna Dong'), ('Adithya Sagar'), ('Xifeng Yan'), ('Paul A. Crook')]",Large Language Models as Zero-shot Dialogue State Tracker through Function Calling,"Large language models (LLMs) are increasingly prevalent in conversational
systems due to their advanced understanding and generative capabilities in
general contexts. However, their effectiveness in task-oriented dialogues
(TOD), which requires not only response generation but also effective dialogue
state tracking (DST) within specific tasks and domains, remains less
satisfying. In this work, we propose a novel approach FnCTOD for solving DST
with LLMs through function calling. This method improves zero-shot DST,
allowing adaptation to diverse domains without extensive data collection or
model tuning. Our experimental results demonstrate that our approach achieves
exceptional performance with both modestly sized open-source and also
proprietary LLMs: with in-context prompting it enables various 7B or 13B
parameter models to surpass the previous state-of-the-art (SOTA) achieved by
ChatGPT, and improves ChatGPT's performance beating the SOTA by 5.6% average
joint goal accuracy (JGA). Individual model results for GPT-3.5 and GPT-4 are
boosted by 4.8% and 14%, respectively. We also show that by fine-tuning on a
small collection of diverse task-oriented dialogues, we can equip modestly
sized models, specifically a 13B parameter LLaMA2-Chat model, with
function-calling capabilities and DST performance comparable to ChatGPT while
maintaining their chat capabilities. We have made the code publicly available
at https://github.com/facebookresearch/FnCTOD"
49,"[('Zixuan Li'), ('Yutao Zeng'), ('Yuxin Zuo'), ('Weicheng Ren'), ('Wenxuan Liu'), ('Miao Su'), ('Yucan Guo'), ('Yantao Liu'), ('Xiang Li'), ('Zhilei Hu'), ('Long Bai'), ('Wei Li'), ('Yidan Liu'), ('Pan Yang'), ('Xiaolong Jin'), ('Jiafeng Guo'), ('Xueqi Cheng')]",KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction,"In this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct
Universal Information Extraction (UIE) via code generation. KnowCoder aims to
develop a kind of unified schema representation that LLMs can easily understand
and an effective learning framework that encourages LLMs to follow schemas and
extract structured knowledge accurately. To achieve these, KnowCoder introduces
a code-style schema representation method to uniformly transform different
schemas into Python classes, with which complex schema information, such as
constraints among tasks in UIE, can be captured in an LLM-friendly manner. We
further construct a code-style schema library covering over $\textbf{30,000}$
types of knowledge, which is the largest one for UIE, to the best of our
knowledge. To ease the learning process of LLMs, KnowCoder contains a two-phase
learning framework that enhances its schema understanding ability via code
pretraining and its schema following ability via instruction tuning. After code
pretraining on around $1.5$B automatically constructed data, KnowCoder already
attains remarkable generalization ability and achieves relative improvements by
$\textbf{49.8%}$ F1, compared to LLaMA2, under the few-shot setting. After
instruction tuning, KnowCoder further exhibits strong generalization ability on
unseen schemas and achieves up to $\textbf{12.5%}$ and $\textbf{21.9%}$,
compared to sota baselines, under the zero-shot setting and the low resource
setting, respectively. Additionally, based on our unified schema
representations, various human-annotated datasets can simultaneously be
utilized to refine KnowCoder, which achieves significant improvements up to
$\textbf{7.5%}$ under the supervised setting."
50,"[('Zhexin Zhang'), ('Junxiao Yang'), ('Pei Ke'), ('Fei Mi'), ('Hongning Wang'), ('Minlie Huang')]",Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization,"While significant attention has been dedicated to exploiting weaknesses in
LLMs through jailbreaking attacks, there remains a paucity of effort in
defending against these attacks. We point out a pivotal factor contributing to
the success of jailbreaks: the intrinsic conflict between the goals of being
helpful and ensuring safety. Accordingly, we propose to integrate goal
prioritization at both training and inference stages to counteract.
Implementing goal prioritization during inference substantially diminishes the
Attack Success Rate (ASR) of jailbreaking from 66.4% to 3.6% for ChatGPT. And
integrating goal prioritization into model training reduces the ASR from 71.0%
to 6.6% for Llama2-13B. Remarkably, even in scenarios where no jailbreaking
samples are included during training, our approach slashes the ASR by half.
Additionally, our findings reveal that while stronger LLMs face greater safety
risks, they also possess a greater capacity to be steered towards defending
against such attacks, both because of their stronger ability in instruction
following. Our work thus contributes to the comprehension of jailbreaking
attacks and defenses, and sheds light on the relationship between LLMs'
capability and safety. Our code is available at
\url{https://github.com/thu-coai/JailbreakDefense_GoalPriority}."
51,"[('Haisu Guan'), ('Huanxin Yang'), ('Xinyu Wang'), ('Shengwei Han'), ('Yongge Liu'), ('Lianwen Jin'), ('Xiang Bai'), ('Yuliang Liu')]",Deciphering Oracle Bone Language with Diffusion Models,"Originating from China's Shang Dynasty approximately 3,000 years ago, the
Oracle Bone Script (OBS) is a cornerstone in the annals of linguistic history,
predating many established writing systems. Despite the discovery of thousands
of inscriptions, a vast expanse of OBS remains undeciphered, casting a veil of
mystery over this ancient language. The emergence of modern AI technologies
presents a novel frontier for OBS decipherment, challenging traditional NLP
methods that rely heavily on large textual corpora, a luxury not afforded by
historical languages. This paper introduces a novel approach by adopting image
generation techniques, specifically through the development of Oracle Bone
Script Decipher (OBSD). Utilizing a conditional diffusion-based strategy, OBSD
generates vital clues for decipherment, charting a new course for AI-assisted
analysis of ancient languages. To validate its efficacy, extensive experiments
were conducted on an oracle bone script dataset, with quantitative results
demonstrating the effectiveness of OBSD. Code and decipherment results will be
made available at https://github.com/guanhaisu/OBSD."
52,"[('Abdelrahman Zayed'), ('Goncalo Mordido'), ('Ioana Baldini'), ('Sarath Chandar')]",Why Don’t Prompt-Based Fairness Metrics Correlate?,"The widespread use of large language models has brought up essential
questions about the potential biases these models might learn. This led to the
development of several metrics aimed at evaluating and mitigating these biases.
In this paper, we first demonstrate that prompt-based fairness metrics exhibit
poor agreement, as measured by correlation, raising important questions about
the reliability of fairness assessment using prompts. Then, we outline six
relevant reasons why such a low correlation is observed across existing
metrics. Based on these insights, we propose a method called Correlated
Fairness Output (CAIRO) to enhance the correlation between fairness metrics.
CAIRO augments the original prompts of a given fairness metric by using several
pre-trained language models and then selects the combination of the augmented
prompts that achieves the highest correlation across metrics. We show a
significant improvement in Pearson correlation from 0.3 and 0.18 to 0.90 and
0.98 across metrics for gender and religion biases, respectively. Our code is
available at https://github.com/chandar-lab/CAIRO."
53,"[('Keqin Peng'), ('Liang Ding'), ('Yancheng Yuan'), ('Xuebo Liu'), ('Min Zhang'), ('Yuanxin Ouyang'), ('Dacheng Tao')]",Revisiting Demonstration Selection Strategies in In-Context Learning,"Large language models (LLMs) have shown an impressive ability to perform a
wide range of tasks using in-context learning (ICL), where a few examples are
used to describe a task to the model. However, the performance of ICL varies
significantly with the choice of demonstrations, and it is still unclear why
this happens or what factors will influence its choice. In this work, we first
revisit the factors contributing to this variance from both data and model
aspects, and find that the choice of demonstration is both data- and
model-dependent. We further proposed a data- and model-dependent demonstration
selection method, \textbf{TopK + ConE}, based on the assumption that
\textit{the performance of a demonstration positively correlates with its
contribution to the model's understanding of the test samples}, resulting in a
simple and effective recipe for ICL. Empirically, our method yields consistent
improvements in both language understanding and generation tasks with different
model scales. Further analyses confirm that, besides the generality and
stability under different circumstances, our method provides a unified
explanation for the effectiveness of previous methods. Code will be released."
54,"[('Mingyu Zheng'), ('Xinwei Feng'), ('Qingyi Si'), ('Qiaoqiao She'), ('Zheng Lin'), ('Wenbin Jiang'), ('Weiping Wang')]",Multimodal Table Understanding,"Although great progress has been made by previous table understanding methods
including recent approaches based on large language models (LLMs), they rely
heavily on the premise that given tables must be converted into a certain text
sequence (such as Markdown or HTML) to serve as model input. However, it is
difficult to access such high-quality textual table representations in some
real-world scenarios, and table images are much more accessible. Therefore, how
to directly understand tables using intuitive visual information is a crucial
and urgent challenge for developing more practical applications. In this paper,
we propose a new problem, multimodal table understanding, where the model needs
to generate correct responses to various table-related requests based on the
given table image. To facilitate both the model training and evaluation, we
construct a large-scale dataset named MMTab, which covers a wide spectrum of
table images, instructions and tasks. On this basis, we develop Table-LLaVA, a
generalist tabular multimodal large language model (MLLM), which significantly
outperforms recent open-source MLLM baselines on 23 benchmarks under held-in
and held-out settings. The code and data is available at this
https://github.com/SpursGoZmy/Table-LLaVA"
55,"[('Yuhao Wang'), ('Yusheng Liao'), ('Heyang Liu'), ('Hongcheng Liu'), ('Yu Wang'), ('Yanfeng Wang')]",MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception,"Recent advancements in Multimodal Large Language Models (MLLMs) have
demonstrated exceptional capabilities in visual perception and understanding.
However, these models also suffer from hallucinations, which limit their
reliability as AI systems. We believe that these hallucinations are partially
due to the models' struggle with understanding what they can and cannot
perceive from images, a capability we refer to as self-awareness in perception.
Despite its importance, this aspect of MLLMs has been overlooked in prior
studies. In this paper, we aim to define and evaluate the self-awareness of
MLLMs in perception. To do this, we first introduce the knowledge quadrant in
perception, which helps define what MLLMs know and do not know about images.
Using this framework, we propose a novel benchmark, the Self-Awareness in
Perception for MLLMs (MM-SAP), specifically designed to assess this capability.
We apply MM-SAP to a variety of popular MLLMs, offering a comprehensive
analysis of their self-awareness and providing detailed insights. The
experiment results reveal that current MLLMs possess limited self-awareness
capabilities, pointing to a crucial area for future advancement in the
development of trustworthy MLLMs. Code and data are available at
https://github.com/YHWmz/MM-SAP."
56,"[('Jinpeng Wang'), ('Bin Chen'), ('Qiang Zhang'), ('Zaiqiao Meng'), ('Shangsong Liang'), ('Shu-Tao Xia')]",Hyperspherical Multi-Prototype with Optimal Transport for Event Argument Extraction,"Deep quantization methods have shown high efficiency on large-scale image
retrieval. However, current models heavily rely on ground-truth information,
hindering the application of quantization in label-hungry scenarios. A more
realistic demand is to learn from inexhaustible uploaded images that are
associated with informal tags provided by amateur users. Though such sketchy
tags do not obviously reveal the labels, they actually contain useful semantic
information for supervising deep quantization. To this end, we propose
Weakly-Supervised Deep Hyperspherical Quantization (WSDHQ), which is the first
work to learn deep quantization from weakly tagged images. Specifically, 1) we
use word embeddings to represent the tags and enhance their semantic
information based on a tag correlation graph. 2) To better preserve semantic
information in quantization codes and reduce quantization error, we jointly
learn semantics-preserving embeddings and supervised quantizer on hypersphere
by employing a well-designed fusion layer and tailor-made loss functions.
Extensive experiments show that WSDHQ can achieve state-of-art performance on
weakly-supervised compact coding. Code is available at
https://github.com/gimpong/AAAI21-WSDHQ."
57,"[('Kanzhi Cheng'), ('Qiushi Sun'), ('Yougang Chu'), ('Fangzhi Xu'), ('Yantao Li'), ('Jianbing Zhang'), ('Zhiyong Wu')]",SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents,"Graphical User Interface (GUI) agents are designed to automate complex tasks
on digital devices, such as smartphones and desktops. Most existing GUI agents
interact with the environment through extracted structured data, which can be
notably lengthy (e.g., HTML) and occasionally inaccessible (e.g., on desktops).
To alleviate this issue, we propose a novel visual GUI agent -- SeeClick, which
only relies on screenshots for task automation. In our preliminary study, we
have discovered a key challenge in developing visual GUI agents: GUI grounding
-- the capacity to accurately locate screen elements based on instructions. To
tackle this challenge, we propose to enhance SeeClick with GUI grounding
pre-training and devise a method to automate the curation of GUI grounding
data. Along with the efforts above, we have also created ScreenSpot, the first
realistic GUI grounding benchmark that encompasses mobile, desktop, and web
environments. After pre-training, SeeClick demonstrates significant improvement
in ScreenSpot over various baselines. Moreover, comprehensive evaluations on
three widely used benchmarks consistently support our finding that advancements
in GUI grounding directly correlate with enhanced performance in downstream GUI
agent tasks. The model, data and code are available at
https://github.com/njucckevin/SeeClick."
58,"[('Peiyi Wang'), ('Lei Li'), ('Liang Chen'), ('Zefan Cai'), ('Dawei Zhu'), ('Binghuai Lin'), ('Yunbo Cao'), ('Qi Liu'), ('Tianyu Liu'), ('Zhifang Sui')]",Large Language Models are not Fair Evaluators,"In this paper, we uncover a systematic bias in the evaluation paradigm of
adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and
compare the quality of responses generated by candidate models. We find that
the quality ranking of candidate responses can be easily hacked by simply
altering their order of appearance in the context. This manipulation allows us
to skew the evaluation result, making one model appear considerably superior to
the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries
with ChatGPT as an evaluator. To address this issue, we propose a calibration
framework with three simple yet effective strategies: 1) Multiple Evidence
Calibration, which requires the evaluator model to generate multiple evaluation
evidence before assigning ratings; 2) Balanced Position Calibration, which
aggregates results across various orders to determine the final score; 3)
Human-in-the-Loop Calibration, which introduces a balanced position diversity
entropy to measure the difficulty of each example and seeks human assistance
when needed. We also manually annotate the ""win/tie/lose"" outcomes of responses
from ChatGPT and Vicuna-13B in the Vicuna Benchmark's question prompt, and
extensive experiments demonstrate that our approach successfully mitigates
evaluation bias, resulting in closer alignment with human judgments. We release
our code and human annotation at \url{https://github.com/i-Eval/FairEval} to
facilitate future research."
59,"[('Jiazhan Feng'), ('Chongyang Tao'), ('Xiubo Geng'), ('Tao Shen'), ('Can Xu'), ('Guodong Long'), ('Dongyan Zhao'), ('Daxin Jiang')]",Synergistic Interplay between Search and Large Language Models for Information Retrieval,"Information retrieval (IR) plays a crucial role in locating relevant
resources from vast amounts of data, and its applications have evolved from
traditional knowledge bases to modern retrieval models (RMs). The emergence of
large language models (LLMs) has further revolutionized the IR field by
enabling users to interact with search systems in natural languages. In this
paper, we explore the advantages and disadvantages of LLMs and RMs,
highlighting their respective strengths in understanding user-issued queries
and retrieving up-to-date information. To leverage the benefits of both
paradigms while circumventing their limitations, we propose InteR, a novel
framework that facilitates information refinement through synergy between RMs
and LLMs. InteR allows RMs to expand knowledge in queries using LLM-generated
knowledge collections and enables LLMs to enhance prompt formulation using
retrieved documents. This iterative refinement process augments the inputs of
RMs and LLMs, leading to more accurate retrieval. Experiments on large-scale
retrieval benchmarks involving web search and low-resource retrieval tasks
demonstrate that InteR achieves overall superior zero-shot retrieval
performance compared to state-of-the-art methods, even those using relevance
judgment. Source code is available at https://github.com/Cyril-JZ/InteR"
60,"[('Jaavid Aktar Husain'), ('Raj Dabre'), ('Aswanth Kumar'), ('Jay Gala'), ('Thanmay Jayakumar'), ('Ratish Puduppully'), ('Anoop Kunchukuttan')]",RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models via Romanization,"This study addresses the challenge of extending Large Language Models (LLMs)
to non-English languages that use non-Roman scripts. We propose an approach
that utilizes the romanized form of text as an interface for LLMs,
hypothesizing that its frequent informal use and shared tokens with English
enhance cross-lingual alignment. Our approach involves the continual
pretraining of an English LLM like Llama 2 on romanized text of non-English,
non-Roman script languages, followed by instruction tuning on romanized data.
The results indicate that romanized text not only reduces token fertility by
2x-4x but also matches or outperforms native script representation across
various NLU, NLG, and MT tasks. Moreover, the embeddings computed on romanized
text exhibit closer alignment with their English translations than those from
the native script. Our approach presents a promising direction for leveraging
the power of English LLMs in languages traditionally underrepresented in NLP.
Our code is available on https://github.com/AI4Bharat/romansetu."
61,"[('Tianyu Chen'), ('Lin Li'), ('Liuchuan Zhu'), ('Zongyang Li'), ('Xueqing Liu'), ('Guangtai Liang'), ('Qianxiang Wang'), ('Tao Xie')]",VulLibGen: Generating Names of Vulnerability-Affected Packages via a Large Language Model,"Security practitioners maintain vulnerability reports (e.g., GitHub Advisory)
to help developers mitigate security risks. An important task for these
databases is automatically extracting structured information mentioned in the
report, e.g., the affected software packages, to accelerate the defense of the
vulnerability ecosystem.
  However, it is challenging for existing work on affected package
identification to achieve a high accuracy. One reason is that all existing work
focuses on relatively smaller models, thus they cannot harness the knowledge
and semantic capabilities of large language models.
  To address this limitation, we propose VulLibGen, the first method to use LLM
for affected package identification. In contrast to existing work, VulLibGen
proposes the novel idea to directly generate the affected package. To improve
the accuracy, VulLibGen employs supervised fine-tuning (SFT), retrieval
augmented generation (RAG) and a local search algorithm. The local search
algorithm is a novel postprocessing algorithm we introduce for reducing the
hallucination of the generated packages. Our evaluation results show that
VulLibGen has an average accuracy of 0.806 for identifying vulnerable packages
in the four most popular ecosystems in GitHub Advisory (Java, JS, Python, Go)
while the best average accuracy in previous work is 0.721. Additionally,
VulLibGen has high value to security practice: we submitted 60 <vulnerability,
affected package> pairs to GitHub Advisory (covers four ecosystems). 34 of them
have been accepted and merged and 20 are pending approval. Our code and dataset
can be found in the attachments."
62,"[('Yifei Wang'), ('Dizhan Xue'), ('Shengjie Zhang'), ('Shengsheng Qian')]",BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents,"With the prosperity of large language models (LLMs), powerful LLM-based
intelligent agents have been developed to provide customized services with a
set of user-defined tools. State-of-the-art methods for constructing LLM agents
adopt trained LLMs and further fine-tune them on data for the agent task.
However, we show that such methods are vulnerable to our proposed backdoor
attacks named BadAgent on various agent tasks, where a backdoor can be embedded
by fine-tuning on the backdoor data. At test time, the attacker can manipulate
the deployed LLM agents to execute harmful operations by showing the trigger in
the agent input or environment. To our surprise, our proposed attack methods
are extremely robust even after fine-tuning on trustworthy data. Though
backdoor attacks have been studied extensively in natural language processing,
to the best of our knowledge, we could be the first to study them on LLM agents
that are more dangerous due to the permission to use external tools. Our work
demonstrates the clear risk of constructing LLM agents based on untrusted LLMs
or data. Our code is public at https://github.com/DPamK/BadAgent"
63,"[('Alena Fenogenova'), ('Artem Chervyakov'), ('Nikita Martynov'), ('Anastasia Kozlova'), ('Maria Tikhonova'), ('Albina Akhmetgareeva'), ('Anton Emelyanov'), ('Denis Shevelev'), ('Pavel Lebedev'), ('Leonid Sinev'), ('Ulyana Isaeva'), ('Katerina Kolomeytseva'), ('Daniil Moskovskiy'), ('Elizaveta Goncharova'), ('Nikita Savushkin'), ('Polina Mikhailova'), ('Denis Dimitrov'), ('Alexander Panchenko'), ('Sergei Markov')]",MERA: A Comprehensive LLM Evaluation in Russian,"Over the past few years, one of the most notable advancements in AI research
has been in foundation models (FMs), headlined by the rise of language models
(LMs). As the models' size increases, LMs demonstrate enhancements in
measurable aspects and the development of new qualitative features. However,
despite researchers' attention and the rapid growth in LM application, the
capabilities, limitations, and associated risks still need to be better
understood. To address these issues, we introduce an open Multimodal Evaluation
of Russian-language Architectures (MERA), a new instruction benchmark for
evaluating foundation models oriented towards the Russian language. The
benchmark encompasses 21 evaluation tasks for generative models in 11 skill
domains and is designed as a black-box test to ensure the exclusion of data
leakage. The paper introduces a methodology to evaluate FMs and LMs in zero-
and few-shot fixed instruction settings that can be extended to other
modalities. We propose an evaluation methodology, an open-source code base for
the MERA assessment, and a leaderboard with a submission system. We evaluate
open LMs as baselines and find that they are still far behind the human level.
We publicly release MERA to guide forthcoming research, anticipate
groundbreaking model features, standardize the evaluation procedure, and
address potential societal drawbacks."
64,"[('Feiteng Fang'), ('Yuelin Bai'), ('Shiwen Ni'), ('Min Yang'), ('Xiaojun Chen'), ('Ruifeng Xu')]",Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training,"Large Language Models (LLMs) exhibit substantial capabilities yet encounter
challenges, including hallucination, outdated knowledge, and untraceable
reasoning processes. Retrieval-augmented generation (RAG) has emerged as a
promising solution, integrating knowledge from external databases to mitigate
these challenges. However, inappropriate retrieved passages can potentially
hinder the LLMs' capacity to generate comprehensive and high-quality responses.
Prior RAG studies on the robustness of retrieval noises often confine
themselves to a limited set of noise types, deviating from real-world retrieval
environments and limiting practical applicability. In this study, we initially
investigate retrieval noises and categorize them into three distinct types,
reflecting real-world environments. We analyze the impact of these various
retrieval noises on the robustness of LLMs. Subsequently, we propose a novel
RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT).
RAAT leverages adaptive adversarial training to dynamically adjust the model's
training process in response to retrieval noises. Concurrently, it employs
multi-task learning to ensure the model's capacity to internally recognize
noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model
trained using RAAT exhibits significant improvements in F1 and EM scores under
diverse noise conditions. For reproducibility, we release our code and data at:
https://github.com/calubkk/RAAT."
65,"[('Wei Jie Yeo'), ('Ranjan Satapathy'), ('Rick Siow Mong Goh'), ('Erik Cambria')]",Prompted Aspect Key Point Analysis for Quantitative Review Summarization,"Prompt Engineering has garnered significant attention for enhancing the
performance of large language models across a multitude of tasks. Techniques
such as the Chain-of-Thought not only bolster task performance but also
delineate a clear trajectory of reasoning steps, offering a tangible form of
explanation for the audience. Prior works on interpretability assess the
reasoning chains yielded by Chain-of-Thought solely along a singular axis,
namely faithfulness. We present a comprehensive and multifaceted evaluation of
interpretability, examining not only faithfulness but also robustness and
utility across multiple commonsense reasoning benchmarks. Likewise, our
investigation is not confined to a single prompting technique; it expansively
covers a multitude of prevalent prompting techniques employed in large language
models, thereby ensuring a wide-ranging and exhaustive evaluation. In addition,
we introduce a simple interpretability alignment technique, termed
Self-Entailment-Alignment Chain-of-thought, that yields more than 70\%
improvements across multiple dimensions of interpretability. Code is available
at https://github.com/SenticNet/CoT_interpretability"
66,"[('Fei Zhu'), ('Xu-Yao Zhang'), ('Zhen Cheng'), ('Cheng-Lin Liu')]",Confidence is not Timeless: Modeling Temporal Validity for Rule-based Temporal Knowledge Graph Forecasting,"Reliable confidence estimation is a challenging yet fundamental requirement
in many risk-sensitive applications. However, modern deep neural networks are
often overconfident for their incorrect predictions, i.e., misclassified
samples from known classes, and out-of-distribution (OOD) samples from unknown
classes. In recent years, many confidence calibration and OOD detection methods
have been developed. In this paper, we find a general, widely existing but
actually-neglected phenomenon that most confidence estimation methods are
harmful for detecting misclassification errors. We investigate this problem and
reveal that popular calibration and OOD detection methods often lead to worse
confidence separation between correctly classified and misclassified examples,
making it difficult to decide whether to trust a prediction or not. Finally, we
propose to enlarge the confidence gap by finding flat minima, which yields
state-of-the-art failure prediction performance under various settings
including balanced, long-tailed, and covariate-shift classification scenarios.
Our study not only provides a strong baseline for reliable confidence
estimation but also acts as a bridge between understanding calibration, OOD
detection, and failure prediction. The code is available at
\url{https://github.com/Impression2805/FMFP}."
67,"[('Lei Kang'), ('Rubèn Tito'), ('Ernest Valveny'), ('Dimosthenis Karatzas')]",PAGED: A Benchmark for Procedural Graphs Extraction from Documents,"Documents are 2-dimensional carriers of written communication, and as such
their interpretation requires a multi-modal approach where textual and visual
information are efficiently combined. Document Visual Question Answering
(Document VQA), due to this multi-modal nature, has garnered significant
interest from both the document understanding and natural language processing
communities. The state-of-the-art single-page Document VQA methods show
impressive performance, yet in multi-page scenarios, these methods struggle.
They have to concatenate all pages into one large page for processing,
demanding substantial GPU resources, even for evaluation. In this work, we
propose a novel method and efficient training strategy for multi-page Document
VQA tasks. In particular, we employ a visual-only document representation,
leveraging the encoder from a document understanding model, Pix2Struct. Our
approach utilizes a self-attention scoring mechanism to generate relevance
scores for each document page, enabling the retrieval of pertinent pages. This
adaptation allows us to extend single-page Document VQA models to multi-page
scenarios without constraints on the number of pages during evaluation, all
with minimal demand for GPU resources. Our extensive experiments demonstrate
not only achieving state-of-the-art performance without the need for Optical
Character Recognition (OCR), but also sustained performance in scenarios
extending to documents of nearly 800 pages compared to a maximum of 20 pages in
the MP-DocVQA dataset. Our code is publicly available at
\url{https://github.com/leitro/SelfAttnScoring-MPDocVQA}."
68,"[('Ying Zhou'), ('Ben He'), ('Le Sun')]",Navigating the Shadows: Unveiling Effective Disturbances for Modern AI Content Detectors,"With the launch of ChatGPT, large language models (LLMs) have attracted
global attention. In the realm of article writing, LLMs have witnessed
extensive utilization, giving rise to concerns related to intellectual property
protection, personal privacy, and academic integrity. In response, AI-text
detection has emerged to distinguish between human and machine-generated
content. However, recent research indicates that these detection systems often
lack robustness and struggle to effectively differentiate perturbed texts.
Currently, there is a lack of systematic evaluations regarding detection
performance in real-world applications, and a comprehensive examination of
perturbation techniques and detector robustness is also absent. To bridge this
gap, our work simulates real-world scenarios in both informal and professional
writing, exploring the out-of-the-box performance of current detectors.
Additionally, we have constructed 12 black-box text perturbation methods to
assess the robustness of current detection models across various perturbation
granularities. Furthermore, through adversarial learning experiments, we
investigate the impact of perturbation data augmentation on the robustness of
AI-text detectors. We have released our code and data at
https://github.com/zhouying20/ai-text-detector-evaluation."
69,"[('Junyi Li'), ('Jie Chen'), ('Ruiyang Ren'), ('Xiaoxue Cheng'), ('Wayne Xin Zhao'), ('Jian-Yun Nie'), ('Ji-Rong Wen')]",The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models,"In the era of large language models (LLMs), hallucination (i.e., the tendency
to generate factually incorrect content) poses great challenge to trustworthy
and reliable deployment of LLMs in real-world applications. To tackle the LLM
hallucination, three key questions should be well studied: how to detect
hallucinations (detection), why do LLMs hallucinate (source), and what can be
done to mitigate them (mitigation). To address these challenges, this work
presents a systematic empirical study on LLM hallucination, focused on the the
three aspects of hallucination detection, source and mitigation. Specially, we
construct a new hallucination benchmark HaluEval 2.0, and designs a simple yet
effective detection method for LLM hallucination. Furthermore, we zoom into the
different training or utilization stages of LLMs and extensively analyze the
potential factors that lead to the LLM hallucination. Finally, we implement and
examine a series of widely used techniques to mitigate the hallucinations in
LLMs. Our work has led to several important findings to understand the
hallucination origin and mitigate the hallucinations in LLMs. Our code and data
can be accessed at https://github.com/RUCAIBox/HaluEval-2.0."
70,"[('Shih-Cheng Huang'), ('Pin-Zu Li'), ('Yu-Chi Hsu'), ('Kuang-Ming Chen'), ('Yu Tung Lin'), ('Shih-Kai Hsiao'), ('Richard Tzong-Han Tsai'), ('Hung-yi Lee')]",Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages,"Recently, the development of open-source large language models (LLMs) has
advanced rapidly. Nevertheless, due to data constraints, the capabilities of
most open-source LLMs are primarily focused on English. To address this issue,
we introduce the concept of $\textit{chat vector}$ to equip pre-trained
language models with instruction following and human value alignment via simple
model arithmetic. The chat vector is derived by subtracting the weights of a
pre-trained base model (e.g. LLaMA2) from those of its corresponding chat model
(e.g. LLaMA2-chat). By simply adding the chat vector to a continual pre-trained
model's weights, we can endow the model with chat capabilities in new languages
without the need for further training. Our empirical studies demonstrate the
superior efficacy of the chat vector from three different aspects: instruction
following, toxicity mitigation, and multi-turn dialogue. Moreover, to showcase
the adaptability of our approach, we extend our experiments to encompass
various languages, base models, and chat vectors. The results underscore the
chat vector's simplicity, effectiveness, and wide applicability, making it a
compelling solution for efficiently enabling conversational capabilities in
pre-trained language models. Our code is available at
https://github.com/aqweteddy/ChatVector."
71,"[('Zhanhui Zhou'), ('Jie Liu'), ('Zhichen Dong'), ('Jiaheng Liu'), ('Chao Yang'), ('Wanli Ouyang'), ('Yu Qiao')]",Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!,"Large language models (LLMs) undergo safety alignment to ensure safe
conversations with humans. However, this paper introduces a training-free
attack method capable of reversing safety alignment, converting the outcomes of
stronger alignment into greater potential for harm by accessing only LLM output
token distributions. Specifically, our method achieves this reversal by
contrasting the output token distribution of a safety-aligned language model
(e.g., Llama-2-chat) against its pre-trained version (e.g., Llama-2), so that
the token predictions are shifted towards the opposite direction of safety
alignment. We name this method emulated disalignment (ED) because sampling from
this contrastive distribution provably emulates the result of fine-tuning to
minimize a safety reward. Our experiments with ED across three evaluation
datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show
that ED doubles the harmfulness of pre-trained models and outperforms strong
baselines, achieving the highest harmful rates in 43 out of 48 evaluation
subsets by a large margin. Eventually, given ED's reliance on language model
output token distributions, which particularly compromises open-source models,
our findings highlight the need to reassess the open accessibility of language
models, even if they have been safety-aligned. Code is available at
https://github.com/ZHZisZZ/emulated-disalignment."
72,"[('Yinya Huang'), ('Ruixin Hong'), ('Hongming Zhang'), ('Wei Shao'), ('Zhicheng Yang'), ('Dong Yu'), ('Changshui Zhang'), ('Xiaodan Liang'), ('Linqi Song')]",CLOMO: Counterfactual Logical Modification with Large Language Models,"In this study, we delve into the realm of counterfactual reasoning
capabilities of large language models (LLMs). Our primary objective is to
cultivate the counterfactual thought processes within LLMs and rigorously
assess these processes for their validity. Specifically, we introduce a novel
task, Counterfactual Logical Modification (CLOMO), and a high-quality
human-annotated benchmark. In this task, LLMs must adeptly alter a given
argumentative text to uphold a predetermined logical relationship. To
effectively evaluate a generation model's counterfactual capabilities, we
propose an innovative evaluation metric, the decomposed Self-Evaluation Score
(SES) to directly evaluate the natural language output of LLMs instead of
modeling the task as a multiple-choice problem. Analysis shows that the
proposed automatic metric aligns well with human preference. Our experimental
results show that while LLMs demonstrate a notable capacity for logical
counterfactual thinking, there remains a discernible gap between their current
abilities and human performance. Code and data are available at
https://github.com/Eleanor-H/CLOMO."
73,"[('Xianfu Cheng'), ('Hang Zhang'), ('Jian Yang'), ('Xiang Li'), ('Weixiao Zhou'), ('Kui Wu'), ('Fei Liu'), ('Wei Zhang'), ('Tao Sun'), ('Tongliang Li'), ('Zhoujun Li')]",Simple but Effective Compound Geometric Operations for Temporal Knowledge Graph Completion,"In the domain of document AI, semi-structured form parsing plays a crucial
role. This task leverages techniques from key information extraction (KIE),
dealing with inputs that range from plain text to intricate modal data
comprising images and structural layouts. The advent of pre-trained multimodal
models has driven the extraction of key information from form documents in
different formats such as PDFs and images. Nonetheless, the endeavor of form
parsing is still encumbered by notable challenges like subpar capabilities in
multi-lingual parsing and diminished recall in contexts rich in text and
visuals. In this work, we introduce a simple but effective \textbf{M}ultimodal
and \textbf{M}ultilingual semi-structured \textbf{FORM} \textbf{PARSER}
(\textbf{XFormParser}), which is anchored on a comprehensive pre-trained
language model and innovatively amalgamates semantic entity recognition (SER)
and relation extraction (RE) into a unified framework, enhanced by a novel
staged warm-up training approach that employs soft labels to significantly
refine form parsing accuracy without amplifying inference overhead.
Furthermore, we have developed a groundbreaking benchmark dataset, named
InDFormBench, catering specifically to the parsing requirements of multilingual
forms in various industrial contexts. Through rigorous testing on established
multilingual benchmarks and InDFormBench, XFormParser has demonstrated its
unparalleled efficacy, notably surpassing the state-of-the-art (SOTA) models in
RE tasks within language-specific setups by achieving an F1 score improvement
of up to 1.79\%. Our framework exhibits exceptionally improved performance
across tasks in both multi-language and zero-shot contexts when compared to
existing SOTA benchmarks. The code is publicly available at
https://github.com/zhbuaa0/layoutlmft."
74,"[('Tianqing Fang'), ('Zeming Chen'), ('Yangqiu Song'), ('Antoine Bosselut')]",Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs,"Event commonsense reasoning requires the ability to reason about the
relationship between events, as well as infer implicit context underlying that
relationship. However, data scarcity makes it challenging for language models
to learn to generate commonsense inferences for contexts and questions
involving interactions between complex events. To address this demand, we
present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop
logical queries (e.g., the joint effect or cause of both event A and B, or the
effect of the effect of event C) from an existing commonsense knowledge graph
(CSKG), and verbalizing them using handcrafted rules and large language models
into multiple-choice and text generation questions. Our experiments show that
language models trained on COM2 exhibit significant improvements in complex
reasoning ability, resulting in enhanced zero-shot performance in both
in-domain and out-of-domain tasks for question answering and generative
commonsense reasoning, without expensive human annotations. Code and data are
available at https://github.com/tqfang/complex-commonsense-reasoning."
75,"[('Florian Le Bronnec'), ('Alexandre Verine'), ('Benjamin Negrevergne'), ('Yann Chevaleyre'), ('Alexandre Allauzen')]",Exploring Precision and Recall to assess the quality and diversity of LLMs,"We introduce a novel evaluation framework for Large Language Models (LLMs)
such as \textsc{Llama-2} and \textsc{Mistral}, focusing on importing Precision
and Recall metrics from image generation to text generation. This approach
allows for a nuanced assessment of the quality and diversity of generated text
without the need for aligned corpora. By conducting a comprehensive evaluation
of state-of-the-art language models, the study reveals new insights into their
performance on open-ended generation tasks, which are not adequately captured
by traditional benchmarks. The findings highlight a trade-off between the
quality and diversity of generated samples, particularly when models are
fine-tuned on instruction dataset or with human feedback. This work extends the
toolkit for distribution-based NLP evaluation, offering insights into the
practical capabilities and challenges that current LLMs face in generating
diverse and high-quality text. We release our code and data."
76,"[('Mouxiang Chen'), ('Hao Tian'), ('Zhongxin Liu'), ('Xiaoxue Ren'), ('Jianling Sun')]",JumpCoder: Go Beyond Autoregressive Coder via Online Modification,"While existing code large language models (code LLMs) exhibit impressive
capabilities in code generation, their autoregressive sequential generation
inherently lacks reversibility. This limitation hinders them from timely
correcting previous missing statements during coding as humans do, often
leading to error propagation and suboptimal performance. We introduce
JumpCoder, a novel model-agnostic framework that enables human-like online
modification and non-sequential generation to augment code LLMs. The key idea
behind JumpCoder is to insert new code into the currently generated code when
necessary during generation, which is achieved through an auxiliary infilling
model that works in tandem with the code LLM. Since identifying the best infill
position beforehand is intractable, we adopt an \textit{infill-first,
judge-later} strategy, which experiments with filling at the $k$ most critical
positions following the generation of each line, and uses an Abstract Syntax
Tree (AST) parser alongside the Generation Model Scoring to effectively judge
the validity of each potential infill. Extensive experiments using six
state-of-the-art code LLMs across multiple and multilingual benchmarks
consistently indicate significant improvements over all baselines. Our code is
public at https://github.com/Keytoyze/JumpCoder."
77,"[('Hamed Hematian Hemati'), ('Hamid Beigy')]",Consistency Training by Synthetic Question Generation for Conversational Question Answering,"Efficiently modeling historical information is a critical component in
addressing user queries within a conversational question-answering (QA)
context, as historical context plays a vital role in clarifying the user's
questions. However, irrelevant history induces noise in the reasoning process,
especially for those questions with a considerable historical context. In our
novel model-agnostic approach, referred to as CoTaH (Consistency-Trained
augmented History), we augment the historical information with synthetic
questions and subsequently employ consistency training to train a model that
utilizes both real and augmented historical data to implicitly make the
reasoning robust to irrelevant history. To the best of our knowledge, this is
the first instance of research using question generation as a form of data
augmentation to model conversational QA settings. By citing a common modeling
error prevalent in previous research, we introduce a new baseline model and
compare our model's performance against it, demonstrating an improvement in
results, particularly when dealing with questions that include a substantial
amount of historical context. The source code can be found on our GitHub page."
78,"[('Ran Xu'), ('Wenqi Shi'), ('Yue Yu'), ('Yuchen Zhuang'), ('Bowen Jin'), ('May D. Wang'), ('Joyce C. Ho'), ('Carl Yang')]",RAM-EHR: Retrieval Augmentation Meets Clinical Predictions on Electronic Health Records,"We present RAM-EHR, a Retrieval AugMentation pipeline to improve clinical
predictions on Electronic Health Records (EHRs). RAM-EHR first collects
multiple knowledge sources, converts them into text format, and uses dense
retrieval to obtain information related to medical concepts. This strategy
addresses the difficulties associated with complex names for the concepts.
RAM-EHR then augments the local EHR predictive model co-trained with
consistency regularization to capture complementary information from patient
visits and summarized knowledge. Experiments on two EHR datasets show the
efficacy of RAM-EHR over previous knowledge-enhanced baselines (3.4% gain in
AUROC and 7.2% gain in AUPR), emphasizing the effectiveness of the summarized
knowledge from RAM-EHR for clinical prediction tasks. The code will be
published at \url{https://github.com/ritaranx/RAM-EHR}."